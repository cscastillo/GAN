{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY59DJbBJr4Q"
      },
      "source": [
        "##Project 2 GAN\n",
        "\n",
        "##### Carlos Santiago Castillo \n",
        "##### 164426\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import keras\n",
        "from keras import Input\n",
        "from keras.layers import Dense, Reshape, LeakyReLU, Conv2D, Conv2DTranspose, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import imageio\n",
        "import shutil\n",
        "import keras.optimizers.schedules"
      ],
      "metadata": {
        "id": "3kjOv-sIf6J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZehWz4xkN1NI"
      },
      "outputs": [],
      "source": [
        "# Load CelebA dataset and split it into train and validation sets\n",
        "celeba_builder = tfds.builder(\"celeb_a\")\n",
        "celeba_builder.download_and_prepare()\n",
        "celeba_train = celeba_builder.as_dataset(split='train')\n",
        "num_images = celeba_builder.info.splits['train'].num_examples\n",
        "\n",
        "# Use only 50% of the images\n",
        "num_images = int(num_images)\n",
        "celeba_train = celeba_train.take(num_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt1ioXmYOF7u"
      },
      "outputs": [],
      "source": [
        "# Define generator model\n",
        "generator = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(256, input_shape=(100000,), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(1024, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(784, activation='tanh'),\n",
        "    tf.keras.layers.Reshape((28, 28, 1))\n",
        "])\n",
        "\n",
        "# Define discriminator model\n",
        "discriminator = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Define loss functions and optimizer\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1\n",
        "    image = tf.image.resize(image, (28, 28))\n",
        "    return image\n",
        "\n",
        "# Function to generate batch of noise vectors\n",
        "def generate_noise(batch_size, noise_dim):\n",
        "    return np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "\n",
        "# Function to generate fake images from noise\n",
        "def generate_fake_images(generator, noise_dim, num_images):\n",
        "    noise = generate_noise(num_images, noise_dim)\n",
        "    fake_images = generator.predict(noise)\n",
        "    return fake_images\n",
        "\n",
        "# Function to train the GAN\n",
        "def train_gan(epochs, batch_size, noise_dim):\n",
        "    # Define labels for real and fake images\n",
        "    real_labels = tf.ones((batch_size, 1))\n",
        "    fake_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for image_batch in celeba_train.batch(batch_size):\n",
        "            # Preprocess images and generate noise\n",
        "            images = preprocess_image(image_batch['image'])\n",
        "            noise = generate_noise(batch_size, noise_dim)\n",
        "\n",
        "            # Generate fake images using generator\n",
        "            with tf.GradientTape() as gen_tape:\n",
        "                fake_images = generator(noise)\n",
        "\n",
        "                # Train generator on fake images\n",
        "                gen_loss = cross_entropy(real_labels, discriminator(fake_images))\n",
        "            gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "            generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
        "\n",
        "            # Train discriminator on real and fake images\n",
        "            with tf.GradientTape() as disc_tape:\n",
        "                real_loss = cross_entropy(real_labels, discriminator(images))\n",
        "                fake_loss = cross_entropy(fake_labels, discriminator(fake_images))\n",
        "                disc_loss = real_loss + fake_loss\n",
        "            disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
        "\n",
        "                # Print loss after each epoch\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}')\n",
        "\n",
        "    # Generate and save sample images after every 100 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        fake_images = generate_fake_images(generator, noise_dim, 16)\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(8, 8), subplot_kw={'xticks': [], 'yticks': []})\n",
        "        for i, ax in enumerate(axes.flat):\n",
        "            ax.imshow(fake_images[i, :, :, 0], cmap='gray')\n",
        "            ax.set_title(f'Image {i+1}')\n",
        "        plt.savefig(f'gan_samples_epoch{epoch}.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "iters = 10*epochs\n",
        "batch_size = 16\n",
        "noise_dim = 1000\n",
        "\n",
        "train_gan(epochs, batch_size, noise_dim)"
      ],
      "metadata": {
        "id": "HfTW2Wdtf_s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx5JJKDTQmED"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fake_images = generate_fake_images(generator, 100000, 1)\n",
        "fig, axes = plt.subplots(4, 4, figsize=(8, 8), subplot_kw={'xticks': [], 'yticks': []})\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(fake_images[i, :, :, 0])\n",
        "    ax.set_title(f'Image {i+1}')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}